# ruHumanEval

## Описание задачи

**Russian HumanEval (ruHumanEval)** является русским аналогом оригинального датасета [HumanEval](https://huggingface.co/datasets/openai_humaneval) [1], созданного для оценки возможностей языковых моделей генерировать код на языке программирования Python для решения простых задач. Датасет направлен на измерение функциональной корректности генерации кода на основе информации из строк документации функции — текстового описания работы функции и нескольких примеров результатов для разных входных данных.

**Важно!** В целях избежания утечки данных, для HumanEval мы создали **НОВУЮ закрытую тестовую часть датасета**, которая по своей структуре и методологии полностью повторяет оригинальный тест, но содержит новые примеры. В связи с этим **результаты моделей на HumanEval и ruHumanEval нельзя напрямую сравнивать между собой.**

**Важно!** Открытые данные это публичный тест ****из оригинального **HumanEval**! Не используйте его для обучения!

**Ключевые слова:** PLP, программирование, Python.

### Мотивация

Данная задача проверяет способность моделей генерировать простые программы на языке Python по описанию (условию) на естественном языке. Так как большие модели имеют в обучающем корпусе долю текстов (программ) написанных на различных языках программирования, предполагается, что они обладают способностью понимать и составлять код для простых задач.

## Описание датасета

### Поля Данных

- instruction — строка, содержащая формулировку запроса к языковой модели;
- inputs — словарь, содержащий входные данные задания:
    - function — строка, содержащая сигнатуру функции, а также ее док-строку в виде недописанной функции;
    - tests — строка со списком словарей, которая содержит входные данные тестовых кейсов для данной задачи (вариантов входных данных, на которых тестируется итоговый код функции);
- outputs — двумерный массив строк размера (n_samples, n_tests), где n_samples - количество сэмплов, требуемое для подсчета метрики pass@k, n_tests - количество тестовых кейсов в tests; каждый список в outputs одинаков и содержит корректные ответы в виде строк на все тестовые кейсы.
- meta — cловарь, содержащий метаинформацию:
    - id — номер примера;
    - canonical_solution — каноническое решение задачи [только в трейн сете];
    - entry_point — имя функции.

### Пример данных

Ниже приведен пример данных:

```jsx
{
	"instruction": "На вход подается функция с описанием в виде строки docstring. В соответствии с описанием вам необходимо реализовать функцию на основе шаблона:\\n{function}"
	"inputs": {
		"function": "
			def greatest_common_divisor(a: int, b: int) -> int:
			'''Верните наибольший общий делитель двух целых чисел a и b.
			Примеры:
			greatest_common_divisor(3, 5)
			1
			greatest_common_divisor(25, 15)
			5
			'''
			",
		"tests": "[{'a': 3, 'b': 7}, {'a': 10, 'b': 15}, {'a': 49, 'b': 14}, {'a': 144, 'b': 60}]"
	},
	"outputs": ["1", "5", "7", "12"]
	"meta": {
		"id": 666, 
		"canonical_solution": "
			def query_gcd(a: int, b: int) -> int:
				return a if b == 0 else query_gcd(b, a % b)
				return query_gcd(a, b)",
		"entry_point": greatest_common_divisor
	},
}
```

### Разбиение данных

Публичный тестсет содержит 164 примера с тестовыми кейсами и ответами, взятых из оригинального датасета. Закрытая тестовая часть содержит 164 задачи с закрытыми ответами, специально собранных в рамках создания данного бенмарка, для которых предоставляются только данные тестовых  кейсов.

### Промпты

Для датасета было подготовлено 10 промптов разной сложности.

Пример промпта:

```jsx
"На вход подается функция с описанием в виде строки docstring. В соответствии с описанием вам необходимо реализовать функцию на основе шаблона:\n{function}."
```

### Создание Датасета

Открытый сет — это публичный открытый тестовый датасет [openai_humaneval](https://huggingface.co/datasets/openai_humaneval) с переведенными на русский язык описаниями условий. В сете исправлены некоторые опечатки в условиях и решениях, а также учтены исправления, описанные в [2].

Тестовый закртый сет был собран из открытых источников вручную по формату оригинального открытого сета и также скорректирован во избежание утечки данных в обучении.

## Оценка

### Метрики

Оценка решения проводится с помощью метрики pass@k, вычисляемой по формуле:

![https://mera.a-ai.ru/storage/editor/sbCvfIHvx7bb4OYHurmj2S53VgKL90aXb7eyKtxx.png](https://mera.a-ai.ru/storage/editor/sbCvfIHvx7bb4OYHurmj2S53VgKL90aXb7eyKtxx.png)

Select an Image

Обозначения: n - общее количество сгенерированных вариантов решений, c - число решений, которые являются корректными, k - выбираемый показатель, сколько вариантов учитываем.

Чтобы оценить pass@k, генерируется n ≥ k вариантов решения для каждой задачи, через которые прогоняются тестовые кейсы (используем n = 10 и k ≤ 10 и в среднем 10 тест-кейсов на задачу), считается количество правильных решений при условии, что всегда c ≤ n. Правильность решения определяется по результатам прохождения модульных тестов, то есть результат прогонов решений  на тест-кейсах должен совпасть с правильными ответами на тест-кейсы одной задачи. Полученная в результате оценка является несмещеной.

### Человеческая оценка

Датасет включает в себя алгоритмические задачи, для решения которых требуется знание языка программирования Python, что является слишком сложной задачей для среднего разметчика. Все задачи имеют строгие решения, потому все метрики человеческой оценки принимаются за 1.0.

## Ссылки

[1] Chen, Mark, et al. "Evaluating large language models trained on code." *arXiv preprint arXiv:2107.03374* (2021).

[2] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation
