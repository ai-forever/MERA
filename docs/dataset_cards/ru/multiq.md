# multiq

## Описание задачи

**MultiQ** - это вопросно-ответный multi-hop датасет для русского языка. Датасет основан на одноименном [датасете](https://tape-benchmark.com/datasets.html#multiq) из бенчмарка TAPE.

Ключевые слова: multi-hop QA, вопросно-ответное задание, знания о мире, логика

**Авторы:** Екатерина Такташева, Татьяна Шаврина, Алена Феногенова, Денис Шавелев, Надежда Катричева, Мария Тихонова, Альбина Ахметгареева, Олег Зинкевич, Анастасия Башмакова, Светлана Иорданская, Алена Спиридонова, Валентина Курешникова, Екатерина Артемова, Владислав Михайлов

## Мотивация

В диагностическом задании мы стремимся ответить на следующий вопрос: могут ли большие языковые модели эффективно перефразировать токсичную и оскорбительную лексику вежливыми альтернативами, сохраняя при этом первоначальный смысл и качество текста? В этом задании оценивается способность модели распознавать и преобразовывать токсичные предложения в более вежливые, что требует глубокого понимания языковых нюансов и умения создавать альтернативные выражения без изменения предполагаемого сообщения. По сути, мы стремимся оценить, насколько хорошо языковые модели могут нормализовывать и улучшать текст для более уважительного общения.

Вопросно-ответные системы всегда играли важную роль в задачах обработки естественного языка. Однако некоторые области , связанные с вопросно-ответными заданиями, все еще являются достаточно сложными для современных моделей. К таким задачам относятся в том числе вопросно-ответные multi-hop задачи. такие как MultiQ.

## Описание датасета

### Поля данных

- meta — словарь, содержащий метаинформацию о примере:
    - id — номер примера в датасете;
    - bridge_answer — список сущностей, необходимых для того чтобы с использованием двух имеющихся текстов дать по ним ответ на вопрос, содержащийся в поле outputs;
- instruction — строка содержащая инструкции для задания;
- inputs — словарь, содержащий следующую информацию:
    - text — строка с основным текстом;
    - support_text — строка с дополнительным текстом;
    - question — вопрос, ответ на который содержится в данных текстах;
- outputs — информация об ответе:
    - label — лейбл ответа;
    - length — длина ответа;
    - offset — индекс начала ответа;
    - segment — строка, содержащая ответ.

### Примеры данных

Каждый пример состоит из двух текстов (основного и дополнительного), а также вопроса по этим текстам, на который необходимо дать правильный ответ.

```jsx
{
 "instruction": "Прочитайте два текста и ответьте на вопрос.\\nТекст 1: {support_text}\\nТекст 2: {text}\\nВопрос: {question}\\nОтвет:",
 "inputs": {
     "question": "В какую реку впадает река, притоком которой является Висвож?",
     "support_text": "Висвож — река в России, протекает по Республике Коми. Устье реки находится в 6 км по левому берегу реки Кыбантывис. Длина реки составляет 24 км.",
     "text": "Кыбантывис (Кабан-Тывис) — река в России, протекает по Республике Коми. Левый приток Айювы. Длина реки составляет 31 км. Система водного объекта: Айюва → Ижма → Печора → Баренцево море."
  },
  "outputs": [{
            "label": "answer",
            "length": 5,
            "offset": 85,
            "segment": "Айювы"
  }],
  "meta": {
      "id": 9,
      "bridge_answers": [{
          "label": "passage",
          "length": 10,
          "offset": 104,
          "segment": "Кыбантывис"
  }]
}
```

### Разбиение данных

Датасет состоит из 1056 обучающих примеров (train set) и 900 тестовых примеров (test set).

### Промпты

Для датасета было подготовлено 5 промптов различной сложности.

Пример:

`"Прочитайте два текста и ответьте на вопрос: {question}\nТекст 1: {support_text}\nТекст 2: {text}\nОтвет:"`

### Создание датасета

Датасет основан на соответствующем датасете из бенчмарка TAPE [1], и был собран из текстов Википедии и WikiData. Полное описание сбора данных можно найти [по ссылке](https://tape-benchmark.com/datasets.html#multiq).

## Оценка

### Метрики

Для оценки моделей на данном датасете используется две метрики: F1 score и полное совпадение (Exact Match - EM).

### Человеческая оценка

Результаты F1 score /EM равны **0**.**92 / 0.91**, соответственно.

## Литература

[1] Taktasheva, Ekaterina, et al. "TAPE: Assessing Few-shot Russian Language Understanding." *Findings of the Association for Computational Linguistics: EMNLP 2022*. 2022.